---
title: "Evaluating privacy and utility of synthetic data"
bibliography: ../references.bib
page-navigation: true
---

::: {.callout-important title="Code so far"}

This section builds on the [previous section](../generation/), and assumes you have completed all exercises. 
You will need the following output.

```{r}
#| label: rerun-analyses
#| results: false
#| message: false
#| warning: false


data <- readRDS(
    url("https://github.com/lmu-osc/synthetic-data-tutorial/raw/refs/heads/main/data/boys.RDS")
)

library(synthpop)
library(densityratio)
library(ggplot2)

syn_param <- syn(
    data = data,
    method = "parametric",
    default.method = c("norm", "logreg", "polyreg", "polr"),
    seed = 123
)

method <- syn_param$method
method["bmi"] <- "~I(wgt/(hgt/100)^2)"
visit <- c("age", "hgt", "wgt", "hc", "gen", "phb", "tv", "reg", "bmi")

syn_passive <- syn(
    data = data,
    method = method,
    visit.sequence = visit,
    seed = 123,
    print = FALSE
)
```

:::

# Synthetic data utility

---

The quality of synthetic data can be evaluated on multiple levels and in different ways.
Often, three types of utility measures are distinguished [@drechsler_30years_2024]:

1. _Fit-for-purpose measures_ are typically the first step in evaluating synthetic data utility, and evaluate for example whether variable types are consistent with the observed data and whether constraints in the observed data are reproduced in the synthetic data (i.e., non-negative quantities, deterministic relationships like between `hgt`, `wgt` and `bmi`).
2. _Global utility measures_ compare the observed and synthetic data on the level of the entire (multivariate) distribution. In theory, if the observed and synthetic data are drawn from the same distribution, any analysis performed on the synthetic data should yield results that are close to results obtained from the observed data. However, in practice, such global measures can be too broad, and high global utility does not guarantee that results of specific analyses are similar between observed and synthetic data.
3. _Outcome-specific utility measures_ evaluate utility for a specific analysis. In the context of synthetic data for open science, it would be helpful if results from the observed data can be approximated closely in the synthetic data. Note that specific utility really focuses on a set of analyses, but does not need to transfer to different analyses.

In isolation, none of these measures typically provide an all-encompassing qualification of utility, also because utility is use-case dependent.
In many instances, the synthetic data modeller does not know what the data will be used for.
In these settings, outcome-specific utility measures are only of limited help, and global utility measures might provide the broadest picture of the utility of the synthetic data.
However, for open science purposes, one could argue that fit-for-purpose measures and outcome-specific utility measures are most important, because in many instances, a data user would want to reproduce the original analyses. 
Note, however, that even if outcome-specific utility is low, ultimately, the synthetic data can be very useful still, as it at least allows to run the original analysis code, and evaluate whether it contains any errors.
In what follows, we briefly discuss all three types of measures.

---

# Fit-for-purpose measures

---

We already evaluated whether the synthetic data looked plausible, and noticed that our synthetic data had some impossible negative values and fixed the relationship between `hgt` and `wgt` for the `bmi` variable in the synthetic data.
We now go one step further, and compare the entire marginal distributions of the variables in the synthetic data with the corresponding variables in the observed data.

---

__1. Compare descriptive statistics from `syn_passive` with descriptive statistics we calculated in the observed data. What do you see?__

:::{.callout-tip title="Show solution" collapse=true}

```{r}
#| label: compare-summary

summary(data)
summary(syn_passive$syn)
```

Most centrality measures are quite close, but the minima and maxima are often substantially off. 
That is, we can observe negative values for `wgt`, `bmi`, `hc` and `tv`. 
Moreover, some exceptionally large values occur for `hgt`, `bmi` and `hc`. 
Also, the categorical variables seem to have somewhat different counts per category in the synthetic data. 
This leaves quite some room for further fine-tuning of our synthesis model. 

:::

---

__2. Use `compare()` from the `synthpop` package to compare the distributions of the observed data with the `syn_passive` data, set the parameters `utility.stats = NULL` and `utility.for.plot = NULL`. What do you see?__

_Note: We set `utility.stats = NULL` and `utility.for.plot = NULL` because here we just focus on the visualizations. We will use different utility statistics later on._

:::{.callout-tip title="Show solution" collapse=true}


```{r}
#| label: compare-synds

compare(
    syn_passive, 
    data, 
    utility.stats = NULL, 
    utility.for.plot = NULL,
    print.flag = FALSE
)
```

Here, we see that we really did not do a great job in preserving the marginal distributions of the variables. 
While for `age`, `hgt` and `wgt` the distributions are close to reasonable (although some problems arise in the tails), the marginal distributions of `bmi`, `hc` and `tv` are really off, with tails that are way too long and certain areas of the distribution that are poorly captured. 
All in all, this suggests that we can do substantially better still when our goal is to model the entire distribution of the observed data.

:::

We further explore the utility of the synthetic data on a multivariate level.
Since visual inspection is typically most insightful, we use the `plot()` function.

---

__3. Plot the variables `age`, `hgt`, `wgt`, `bmi`, `hc` and `tv` against each other, by calling `plot()` on the subset of the data containing these variables. Do the same for the synthetic data.__

:::{.callout-tip title="Show solution" collapse=true}

```{r}
#| label: plot-bivariate

plot(
    data[,c("age", "hgt", "wgt", "bmi", "hc", "tv")],
    main = "Observed data"
)
plot(
    syn_passive$syn[,c("age", "hgt", "wgt", "bmi", "hc", "tv")],
    main = "Synthetic data"
)
```

We see that the distributions of the synthetic data are much more noisy than the distributions of the observed data. 
Moreover, non-linear relationships are not captured, and the relationships of all variables with `hc` and `tv` really seem quite off.
So, there are really some improvements that we can still make.
:::

---

Now that we investigated fit-for-purpose utility, we must decide whether the synthetic data is of sufficient quality.
By now, you probably suspect that this depends on the use-case at hand.
You have seen that some measures of utility indicate substantial problems with the synthetic data, in the sense that certain aspects of the distribution of the data are poorly modelled. 
At the same time, this is not to say that the synthetic data is useless, there is already quite some information in the synthetic data and there is quite a lot that we can do with it already. 
We will come back to this in the [section on outcome-specific utility measures](@sec-outcome-utility).
We decide that we attempt to further improve the utility of the synthetic data.

# Further synthesis improvements

To improve the utility of the synthetic data, we use regression trees for the continuous variables (the `"cart"` model in `synthpop`), which is a non-parametric method that is better able to capture non-linear relationships between variables.
In short, `cart` repetitively splits the predictor space according to which predictor is best able to predict the outcome, leading to so-called _leaves_ in which subsets of the outcome with relatively similar values are collected [see, e.g., Section 8.1.1 in @islr].
Synthetic data is then obtained by splitting the predictors of the synthetic cases according to the learned splits, after which values are sampled from the leaves.
Because this approach recycles observed data, we add some smoothing, so that the observed values are not exactly reproduced in the synthetic data.
This smoothing is often essential from a privacy-perspective, because it makes sure that synthetic values are not exactly equal to the observed data values.

---

__4. Adjust the previously created method vector by replacing every instance of `"norm"` with `"cart"`, and call `syn()` with this new method vector and `smoothing = "spline"`.__

_You may again use `seed = 123` to replicate our results._

:::{.callout-tip title="Show solution" collapse=true}

```{r}
#| label: syn-cart

method[method == "norm"] <- "cart"

syn_cart <- synthpop::syn(
    data,
    method = method,
    smoothing = "spline",
    seed = 123,
    print.flag = FALSE
)
```

We once more get the message that the relationship for `bmi` does not hold in the observed data, but we ignore this again.

:::

---

__5. Check whether the synthetic data looks okay using the `compare()` function that we used previously.__

:::{.callout-tip title="Show solution" collapse=true}

```{r}
#| label: compare-synds-cart

compare(
    syn_cart, 
    data, 
    utility.stats = NULL, 
    utility.for.plot = NULL,
    print.flag = FALSE
)
```

Marginally, we see that the distributions of the variables are a lot better than in our previous synthesis with `norm`. This looks promising.

:::

---

__6. Plot the variables `age`, `hgt`, `wgt`, `bmi`, `hc` and `tv` against each other, by calling `plot()` on `syn_cart$syn`.__

:::{.callout-tip title="Show solution" collapse=true}

```{r}
#| label: plot-bivariate-cart

plot(
    data[,c("age", "hgt", "wgt", "bmi", "hc", "tv")],
    main = "Observed data"
)
plot(
    syn_cart$syn[,c("age", "hgt", "wgt", "bmi", "hc", "tv")],
    main = "Synthetic data"
)
```

Using `cart`, the synthetic data looks substantially more like the observed data than the previously created synthetic data, because it is better able to reproduce non-linear relationships between variables.

:::

We now have two synthetic data sets that seem to have different levels of utility.
We will further explore this using global and outcome-specific utility measures.

---

# Global utility measures

---

We evaluate global utility by comparing the distributions of the synthetic data sets with the distribution of the observed data.
This is often done using one of two ways. 

1. The first is to compare the two distributions using some discrepancy measure. The smaller the discrepancy is, the more the synthetic data looks like the observed data.
2. Another method is to train a classification model to distinguish between the observed and synthetic data, a method commonly called the propensity mean squared error [$pMSE$\; for more information about this method, see @snoke_utility_2018]. The easier it is for the classifier to distinguish between the observed and synthetic data, the lower the global utility, because this implies that the observed and synthetic data are different on some aspects.

We discuss the former approach, as it performed better for evaluating synthetic data utility according to @volker_density_2024.
Still, the $pMSE$ is also used relatively often and can also yield insight in the global utility of a synthetic data set.

---

## Divergence estimation through density ratio estimation

One way to evaluate global utility is by comparing the distributions of the observed and synthetic data directly. 
This can be done using density ratio estimation, which, like the name suggests, attempts to estimate the ratio of the densities of two groups of samples (note that we mean the same thing with distribution and density, although some nuances exist).
That is, we attempt to estimate
$$
r(X) = \frac{f_\text{obs}(X)}{f_\text{syn}(X)}.
$$
This can be done by estimating the distributions of $f_\text{obs}(X)$ and $f_\text{syn}(X)$, but estimating $r(X)$ directly typically yields better results.
The `R`-package `densityratio` has been developed for this purpose [@densityratio].
Based on the density ratio, we can estimate various divergence measures between the two data sets, which is an in-built functionality.

---

__7. Compare the observed and synthetic data with the `ulsif()` function from the `densityratio` package. Do this for both synthetic data sets.__

_Note: Set `numerator_data = data` and `denominator_data = syn_passive$syn` first, and `denominator_data = syn_cart$syn` second._

:::{.callout-tip title="Show solution" collapse=true}

```{r}
#| label: fit-ulsif

r_passive <- ulsif(data, syn_passive$syn)
r_cart <- ulsif(data, syn_cart$syn)
```

The `ulsif()` function performs cross-validation to find the optimal model to estimate the density ratio, so that typically no model specification on behalf of the user is required.

:::

__8. Call `plot()` on the estimated model objects. This displays the distributions of the estimated density ratio values for the observed and synthetic data.__

_Hint:_ The more these distributions overlap, the higher the similarity between the observed and synthetic data distributions.

:::{.callout-tip title="Show solution" collapse=true}

```{r}
#| label: plot-dr-dist

plot(r_passive)
plot(r_cart)
```

You will see that the density ratio values of the `syn_passive` data overlap much less with the density ratio values of the observed data than the density ratio values of the `syn_cart` values, indicating higher utility.

:::

---

__9. Call `summary()` on the density ratio objects to obtain the estimated Pearson divergence between the observed and synthetic data sets.__

:::{.callout-tip title="Show solution" collapse=true}

```{r}
#| label: summary-ulsif

summary(r_passive)
summary(r_cart)
```

The Pearson divergence of the `syn_cart` data is substantially smaller than the Pearson divergence of the `syn_passive` data, indicating higher global utility of the former.

:::

Finally, we can plot the estimated density ratios against the variables in the synthetic data, to see where the synthetic data does not fit the observed data.

---

__10. Use `plot_bivariate()` on the fitted density ratio models, and set `grid = TRUE` and `samples = "denominator"` to only display the synthetic data points.__

_Hint:_ Very dark blue values indicate that synthetic data points in those regions are rather scarce in the observed data, whereas dark red values indicate that in some regions, the synthetic data is underrepresented.

:::{.callout-tip title="Show solution" collapse=true}

```{r}
#| label: plot-bivariate-dr

plot_bivariate(r_passive, grid = TRUE, samples = "denominator")
plot_bivariate(r_cart, grid = TRUE, samples = "denominator")
```

The visualizations again show that the `syn_passive` method yielded synthetic data in regions where it was not so much expected, and that the `cart` method produces synthetic data with higher utility.

:::

# Outcome-specific utility {#sec-outcome-utility}

For outcome-specific utility, we need at least one analysis that we expect data users will be interested in.
For now, we assume that the data users will be interested in predicting head circumference (`hc`), based on `age`, height (`hgt`), weight (`wgt`) and region (`reg`).
Additionally, we will fit a regression analysis where we add quadratic terms for `age` and `hgt`. 
High outcome-specific utility implies that the regression coefficients obtained from the synthetic data are close to the coefficients estimated from the observed data.
One way to measure "closeness" in this context is via the confidence interval overlap. 
The confidence interval overlap quantifies to what extent the confidence intervals of the observed and synthetic coefficients overlap:
$$
CIO = \frac{1}{2} \Big(
    \frac{\min (U_\text{obs}, U_\text{syn}) - 
    \max (L_\text{obs}, L_\text{syn})}{
        U_\text{obs} - L_\text{syn}
    } + \frac{\min (U_\text{obs}, U_\text{syn}) - 
    \max (L_\text{obs}, L_\text{syn})}{
        U_\text{syn} - L_\text{obs}
    }  \Big).
$$
The terms $U$ and $L$ denote the upper and lower bounds of the $95\%$ confidence intervals for the observed and synthetic data.
Visually, the $CIO$ can be depicted as follows:
```{r}
#| label: CIO-fig
#| echo: false
#| message: false
#| warning: false
#| fig-width: 9
#| fig-height: 4

ggplot() +
    geom_point(
        aes(x = c(0.1, 0.5), y = c(0.5, 0.5), col = "Observed")
    ) +
    geom_point(
        aes(x = c(-0.1, 0.4), y = c(0.45, 0.45), col = "Synthetic")
    ) +
    geom_text(
        aes(x = c(0.1, 0.5), 
            y = c(0.5, 0.5), 
            label = c(paste(expression(L[obs])), 
                      paste(expression(U[obs]))),
            col = "Observed"),
        parse = TRUE,
        hjust = -0.1,
        vjust = -0.15
    ) +
    geom_text(
        aes(x = c(-0.1, 0.4), 
            y = c(0.45, 0.45), 
            label = c(paste(expression(L[syn])), 
                      paste(expression(U[syn]))),
            col = "Synthetic"),
        parse = TRUE,
        hjust = -0.1,
        vjust = -0.15
    ) +
    geom_segment(
        aes(y = 0.5, x = 0.1, xend = 0.5, col = "Observed"),
        lineend = "square"
    ) +
    geom_segment(
        aes(y = 0.45, x = -0.1, xend = 0.4, col = "Synthetic"),
        lineend = "square"
    ) +
    geom_vline(aes(xintercept = c(0.1, 0.4)), linetype = 2) +
    ylim(0.35, 0.6) +
    xlim(-0.15, 0.55) +
    scale_color_brewer(palette = "Set1") +
    theme_minimal() +
    labs(x = NULL, y = NULL, col = NULL)
```

The length between the dashed vertical lines is the quantity in the numerator, whereas the lengths of the two horizontal lines are in the denominators.
If the two confidence intervals are almost the same, the confidence interval overlap is large, and the difference between coefficients is relatively small compared to the uncertainty around the coefficients. 
Conversely, if the overlap is small (or even negative), the coefficients are relatively far apart, relative to the random variation one would expect.


---

__11. Fit a linear regression model with main effects only using `lm.synds()` for both synthetic data sets. Compare the results with the observed data using `compare.fit.synds()`.__

:::{.callout-tip title="Show solution" collapse=true}

```{r}
#| label: fit-main-models

fit_passive <- lm.synds(
    hc ~ age + hgt + wgt + reg, 
    data = syn_passive
)

fit_cart <- lm.synds(
    hc ~ age + hgt + wgt + reg, 
    data = syn_cart
)

compare.fit.synds(fit_passive, data = data)
compare.fit.synds(fit_cart, data = data)
```

The results show that the `syn_passive` data yields very similar outcome-specific utility as the `syn_cart` data, even though previous utility measures showed much higher utility for `syn_cart`.
The output shows both the regression coefficients in the synthetic and observed data, along with the raw and standardized difference between these coefficients and the confidence interval overlap.
Further, the results show the Mahalanobis distance ratio (over the expected Mahalanobis distance between observed and synthetic data under a correct synthesis model), and the corresponding test statistic.
The corresponding figure shows the confidence intervals for the observed and synthetic data coefficients.
For both synthesis methods, the confidence intervals overlap substantially with those obtained in the real data, suggesting that the differences between the estimates are relatively small compared to the uncertainty around the regression coefficients (i.e., the sampling error). 

:::

---

This illustration has important consequences! 
If you expect that the synthetic data users are solely interested in analyses on main effects (i.e., regression coefficients in linear models, or mean-differences in t-tests), modelling solely these main effects appropriately is typically sufficient to obtain reasonable synthetic data. 
Even if the marginal (univariate) distributions are poorly captured, many standard analyses can still be conducted.
Moreover, modelling every aspect of the synthetic data is thus often not required to obtain useful synthetic data. 
Additionally, this analysis shows that low global utility does not necessarily mean that outcome-specific utility will also be low. 
The reverse holds equivalently, though not in this dataset: high global utility also does not automatically render high outcome-specific utility.
It is often helpful to be explicit about what the synthetic data can and cannot do, and this is easier with relatively simple synthesis models, such as the parametric models used in `syn_passive` [see also [the section on publishing synthetic data](/publishing/)].
More complex non-parametric models are inherently more of a black-box, and it can be surprising what comes out, even if you carefully evaluated its utility.


---

__12. Fit a linear regression model with the above specification using `lm.synds()` for both synthetic data sets, but now add the quadratic effects for `age` and `hgt`. Compare the results with the observed data using `compare.fit.synds()`.__

:::{.callout-tip title="Show solution" collapse=true}

```{r}
#| label: fit-models

fit_passive_qd <- lm.synds(
    hc ~ age + I(age^2) + hgt + I(hgt^2) + wgt + reg, 
    data = syn_passive
)

fit_cart_qd <- lm.synds(
    hc ~ age + I(age^2) + hgt + I(hgt^2) + wgt + reg, 
    data = syn_cart
)

compare.fit.synds(fit_passive_qd, data = data)
compare.fit.synds(fit_cart_qd, data = data)
```


Here, we see that the coefficients of the `syn_passive` data are substantially off. 
This is no surprise, as we did not include these quadratic effects in our synthesis model.
Since the variables obtained from the quadratic transformations are typically correlated with the variables on the original scale, this typically biases the regression coefficients.
This is exactly how we modelled the synthetic data, and we thus knew what we could expect.
The coefficients of `syn_cart` look much better, and have substantial confidence interval overlap with those obtained in the true data.
So, in these scenarios, the `syn_cart` data really shows much higher utility in essentially every respect, as compared to the parametric synthesis with passive imputation.


:::

---

# Statistical disclosure control

---

Synthetic data can provide a relatively safe framework for sharing data. However, some risks will remain present, and it is important to evaluate these risks. For example, it can be the case that the synthesis models were so complex that the synthetic records are very similar or even identical to the original records, which can lead to privacy breaches.

:::{.callout-warning title = "Privacy of synthetic data"}
Synthetic data by itself does not provide any formal privacy guarantees. These guarantees can be incorporated, for example by using differentially private synthesis methods. Without going into the details, differential privacy starts from the point that every individual in a data set contributes some information to the parameters of a model, from which an adversary might learn something about the sampled individuals. Differential privacy bounds the influence a single observation can have on the synthesis model, and adds noise to the model parameters that is proportional to this influence. That is, if a single individual can have a large influence on the model parameters, more noise is added to protect the privacy of this single individual. For an accessible introduction to differential privacy, the blog by @desfontainesblog20180816 is highly recommended. @bowen_dp_synthesis_2020 reviews differentially private synthesis methods. However, these methods are not yet widely available in R. If privacy is not built-in by design, it remains important to inspect the synthetic data for potential risks. Especially if youâ€™re not entirely sure, it is better to stay at the safe side: use relatively simple, parametric models, check for outliers, and potentially add additional noise to the synthetic data. See also Chapter 4 in the book [Synthetic Data for Official Statistics](https://unece.org/statistics/publications/synthetic-data-official-statistics-starter-guide).
:::

We focus the evaluation of disclosure again on identity disclosure and attribute disclosure, as discussed in the [Statistical Disclosure Control section](../sdc/).
In short, identity disclosure implies that it is possible to identify records in the synthetic data from a set of known characteristics. 
Identity disclosure is not typically possible if the entire dataset is comprised of solely synthetic values, but it is still an important aspect of attribute disclosure. 
Attribute disclosure occurs when it is possible to infer new information from a set of known characteristics.
For example, if all individuals above a certain age in some region have a given disease, one might conclude that someone satisfying these constraints also has that disease.

We evaluate identity disclosure using the _replicated uniques_ measure, which refers to unique observations in the synthetic data that were also unique cases in the observed data [on a set of identifying variables\; @raab_disclosure_2024].
These identifying variables must be specified by synthetic data modeller.
The idea is that observations that have a unique set of identifying variables can be identified by a third party.
If these identifying variables re-occur in the synthetic data, and correspond to an actual observation, a third party might conclude that a particular individual is sampled. 
At the same time, if the synthetic values on at least one of the non-identifying, but potentially sensitive, variables are also the same or very close to the actual values, this might allow for attribute disclosure.
These replicated uniques can be removed from the synthetic data, which might not reduce the utility of the synthetic data too much.

Attribute disclosure is measured using a prediction model, and essentially asks the question whether we can correctly predict some target variable from a set of identifying variables. 
If this is possible in general, knowing someone's values on a set of identifying variables allows to infer potentially sensitive information on the target variable.
In `synthpop`, this procedure is implemented as follows [@raab_disclosure_2024]:

- First, we check which combinations of the identifying variables in the observed data also occur in the synthetic data. 
- Subsequently, we evaluate whether the records with the same values on the identifying variables in the synthetic data also have the same values (or very similar values) on the target variable.
- Finally, we check whether this value corresponds to the actual value on the target variable in the original data.
The proportion of records that meets each of these criteria is referred to as _DiSCO_ (Disclosive in Synthetic, Correct in Original).

Both methods (_replicated uniques_ and _DiSCO_) can be evaluated in `synthpop` using the `multi.disclosure()` function.

---

__13. Call the function `multi.disclosure()` on the `syn_passive` and `syn_cart` data sets. Use `age`, `hgt` and `reg` as identifying variables (the `keys` argument).__

:::{.callout-tip title="Show solution" collapse=true}

```{r}
#| label: sdc

multi.disclosure(syn_passive, data, keys = c("age", "hgt", "reg"))
multi.disclosure(syn_cart, data, keys = c("age", "hgt", "reg"))
```

From the output, it can be seen that disclosure risk is very low for both synthesis methods, according to the criteria defined here. 
For both synthetic data sets, there are no replicated uniques, and the figures likewise show that the attribute disclosure risk is equal to zero for both data sets. 
Note that here we used a subset of the variables we deemed identifying, but this differs per data set, and should also be considered per data set.
The same holds for the potentially identifying variables. 
For the sake of illustration, I chose `age`, `hgt` and `reg`, but depending on the use case, different choices might be reasonable.

:::

After evaluating disclosure risks, it is possible to perform post-processing of the synthetic data. 
This procedure can be streamlined using the `sdc()` function in `synthpop`.
For example, one might remove the _replicated uniques_ from the synthetic data, using the `rm.replicated.uniques` argument in `sdc()`. 
Additionally, it is possible to cap outliers using the `bottom.top.coding` argument or to smooth variables using `smooth.vars`.

In our case, we already applied smoothing, and there are not really any extreme outliers present. 
Therefore, the `sdc()` function cannot do much more to further enhance our privacy protection.
For the sake of illustration, we show how to apply the `sdc()` function below anyhow.

---

__14. Apply the `sdc()` function with `keys = c("age", "hgt", "reg")` and set `rm.replicated.uniques = TRUE` and store the results in `syn_passive_sdc`.__

_Hint:_ This function updates the synthetic data object to remove duplicated records from the observed data (if these records were unique in the original data), make sure to release the synthetic data from the updated object if you use this function.

:::{.callout-tip collapse="true" title="Show output"}

```{r}
#| label: apply-sdc

syn_passive_sdc <- sdc(
    syn_passive, 
    data = data,
    keys = c("age", "hgt", "reg"),
    rm.replicated.uniques = TRUE,
)
```

:::

---

# Conclusion

---

In this tutorial, you have learned to create and evaluate synthetic data.
We discussed modelling deterministic relationships, refining synthesis models based on utility measures, comparing utility of synthetic data sets, and evaluating remaining disclosure risks. 
Remember that if you plan to release synthetic data for open science purposes, it is better to stay on the safe side with respect to privacy. 
When in doubt, choose the option that yields the smallest risk.
We will share some advice on publishing synthetic data in the [next section](/publishing/).


::: {.callout-tip}

## Summary

- Synthetic data utility can be evaluated from multiple perspectives, including fit-for-purpose utility, global distributional similarity and outcome-specific utility.
- Visual comparisons of observed and synthetic data provide an intuitive first assessment of utility, but can be complemented by additional quantitative measures.
- Simpler parametric models may produce implausible values, but can still yield decent outcome-specific utility.
- More flexible synthesis models, such as CART, can better capture non-linear relationships and improve utility for specific analyses, but typically increase disclosure risk. 
- When in doubt, prioritize privacy. Synthetic data are best suited for exploratory analyses, code development and reproducibility checks, rather than definitive inference, and utility is therefore always of secondary importance only.

:::